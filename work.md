---
layout: work
title: Publications and Preprints
slug: /work
items:
  - title: Is Functional Correctness Enough to Evaluate Code Language Models? Exploring Diversity of Generated Codes
    authors: <strong>*Heejae Chon</strong>, *Seonghyeon Lee, Jinyoung Yeo, Dongha Lee
    conference: Preprint, 2024
    paper_url: https://arxiv.org/abs/2408.14504
    image:
      src: /assets/img/work/function.png
      alt: water
    description: This paper emphasizes the importance of code diversity, beyond functional correctness, in evaluating code language models (LMs). We propose a framework that includes novel metrics for inter-code similarity and introduce a pairwise similarity measure that correlates well with human judgment. We show that while current LMs generate functionally correct code, they often lack diversity, which is essential for robust and varied code solutions.
  - title: Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation
    authors: Seonghyeon Lee, Suyeon Kim, Joonwon Jang, <strong>Heejae Chon</strong>, Dongha Lee, Hwanjo Yu
    conference: EMNLP, 2024
    paper_url: https://arxiv.org/abs/2409.13928
    image:
      src: /assets/img/work/elict.png
      alt: sand
    description: The paper explores how instruction-tuned models can effectively utilize auxiliary functions in code generation. By designing prompts that encourage models to use auxiliary functions, the authors show improved performance compared to models without this functionality. Their method surpasses even powerful proprietary models like GPT-4, highlighting the potential of open-source models when auxiliary function usage is integrated
---
